Autonomous systems are expected to complete increasingly more complex missions in dynamic and uncertain environments. In many domains however, these missions are often limited by communication or sensing restrictions. Consider a Mars rover that has been tasked to safely explore and coordinate with a scouting helicopter as proposed in \cite{landau2015helicopter} for the upcoming 2020 Mars rover mission. Missions of such sophisticated nature will necessitate on-board autonomy \cite{francis2017advanced,estlin2007increased}. There are, however, tight sensing restrictions due to the power consumption of on-board sensors and transmitters. Furthermore, there are bandwidth constraints on data sent from the Earth and orbiting satellites \cite{sherwood2014,Backes1999}. In these cases it is necessary to have controllers that can make use of \emph{limited information} and still complete their mission specification above an acceptable threshold. %In some situations, there is not enough time to transmit all sensor data before a control decision can be made. For example, in distributed control of UAVs that may need to react quickly to changes in the environment \cite{Baillieul07}. 

In this paper, we model the environment and plant dynamics using a Markov decision process (MDP). MDPs are one of the most widely studied models for decision-making under uncertainty in the fields of artificial intelligence, robotics, and optimal control \cite{Papadimitriou87,Fu15}. Hence, understanding the effects of limiting the information flow to the policy synthesizer is key towards pushing the use of MDPs in information-constrained applications like Mars rovers. This will allow us to take advantage of the existing well studied techniques in controlling MDPs for use in communication and sensing constrained situations. 

Defining sophisticated mission objectives in MDPs requires specifying complex reward functions in order to synthesize a control policy \cite{puterman2014}. Alternatively, temporal logic has been used as a formal way to allow the user to more intuitively specify high-level specifications such as infinitely often patrolling a region or moving through certain regions in a specific order. Linear temporal logic (LTL) in particular has been popular for use in optimal control \cite{Svoreňová13,Fu15} as it is widely studied and several tools exist to synthesize provably correct policies in MDPs. %It is known that deriving controllers for rich temporal logic specifications reduces to solving a reachability problem on an MDP. 
The focus of related work in this field is in cases where full state information is \emph{unavailable}, \ie where only part of the state can be observed, whereas we look at scenarios where the information is available, but with restricted access. Specifically, we study the effect of information restriction on satisfying temporal logic objectives in MDPs.

In order to study the effects of restricted information flow, we need to be able to quantify it. We use \emph{transfer entropy} \cite{schreiber2000} to quantify the directional information flow between two random processes, for example, from the state of an MDP to the control policy. Intuitively minimizing the transfer entropy promotes policies that rely less on knowledge of the current state of the system. In communication theory, a related quantity called \emph{directed information} has been used to measure channel capacities in feedback systems \cite{massey1990causality,tatikonda2009capacity} as well as a proxy for feedback data rate to controllers \cite{silva2011achievable}. However, transfer entropy is often used for studying causality in fields including statistical thermodynamics \cite{parrondo2015thermodynamics}, neuroscience \cite{vicente2011transfer}, and biology \cite{tung2007inferring}. Transfer entropy has also been previously used with policy synthesis in MDPs in \cite{takashi17}, though without temporal logic specifications. 

There has been a lot of work on quantifying information requirements for low-level control requirements like stability \cite{Nair07}. However, quantifying information requirements for high level decision making scenarios that we are interested in are not as widely studied. This is analogous to model reduction techniques for MDPs studied in \cite{Bharadwaj17,brazdil2014verification,ciesinski2008reduction}, where states and actions that are completely irrelevant to the mission are removed. However, information use is not quantified so there may still be an overreliance on the existing information. ~\cite{Tishby2011} examines directed information in MDPs to quantify information. Policies are penalized if they vary too much from a completely uninformed starting point, e.g, take any action with equal probability. We are, however, interested in studying the causality of information from the state to the controller, i.e, we seek to not send information that is not needed for the decision-making process. This property makes transfer entropy a better information-theoretic metric for our setting. Our work can be seen as a generalization of \cite{takashi17}, where the transfer entropy is also used, but the authors do not allow the possibility of penalizing certain sensors over others. This is crucial in settings where different sensors can have varying energy costs and is hence necessary to budget the use of certain sensors to conserve energy. Furthermore, in distributed control of multiple agents, it is sensible to reduce the reliance of information transmitted from other agents compared to information from on-board sensors due to bandwidth restrictions in transmission.

\paragraph*{\textbf{Contributions}} We develop formal means to connect information-theoretic techniques for policy synthesis in MDPs with techniques from formal methods and probabilistic model checking. Specifically, we include a transfer entropy cost in that we  minimize along with probability of failure of satisfying a probability specification. 

In contrast to standard MDP policy computation under temporal logic specifications, the additional information cost leads to randomized optimal policies \cite{tanaka2017lqg,Todorov09,takashi17}. This necessitates policy search in an infinite state space. To solve this efficiently, we derive a sufficient optimality condition in the form of coupled non-linear equations. We solve this using a modified version of a forward-backward iterative algorithm from ~\cite{Blahut72}. While the proposed method builds on earlier results in \cite{takashi17}, we generalize the setting to penalize subsets of state variables and incorporate rich temporal logic constraints. 

Additionally, in the application of networked control systems, we provide a physical interpretation of the transfer entropy cost. We prove that it is in fact the lower bound on the data rate of the penalized state variables that must be transmitted.

Finally, we present a path planning example in grid worlds with moving and static obstacles and qualitatively analyze the impact of the information cost on relevant state variables.

% Control systems using multiple distributed sensors and/or processors has become increasingly popular in recent times \cite{Baillieul07}. In such scenarios, understanding communication restrictions and delays is imperative for real world applications. Indeed, a panel on future directions in control had identified control under communication restrictions to be an important future direction \cite{Murray03}.

% Accounting for cost of information is crucial in many decision-making scenarios. For example, a planetary rover may need to communicate with a ground station or orbiting satellite in order to complete its mission \cite{Backes1999}. However, there are often tight bandwidth constraints so it is necessary to only transmit the minimal amount of information possible to achieve the required performance \cite{sherwood2014}. Another example is coordinating autonomous agents in a hostile environment. In these cases it can be desirable to restrict communication to avoid detection, as well as to save power. Often, there is not enough time to transmit all sensor data before a control decision can be made such as in distributed control of UAVs that may need to react quickly to changes in the environment \cite{Baillieul07}. In these situations it is necessary to send the minimal information possible while still guaranteeing a certain amount of performance.

% Specifically, we want to synthesize a control policy in a Markov decision process (MDP) that minimizes the information reliance from the state but still performs above a minimum prescribed performance threshold. MDPs are one of the most widely studied tools for decision making under uncertainty in the fields of artificial intelligence, and robotics \cite{Papadimitriou87,Bharadwaj17} for optimal control and motion planning \cite{Ragi13,Burlet04}, so having a better understanding of information flow to controllers is key to pushing our understanding of communication restricted control. 

% We use \emph{transfer entropy} \cite{schreiber2000} to quantify the directional information flow between two random processes. Intuitively minimizing the transfer entropy promotes policies that rely less on the current state of the system. In communication theory, a related quantity called \emph{directed information} has been used to measure channel capacities in feedback systems \cite{massey1990causality,tatikonda2009capacity} as well as in studying feedback data rate to controllers \cite{silva2011achievable}. However, transfer entropy is often used when causality is being studied, usually in fields like statistical thermodynamics \cite{parrondo2015thermodynamics}, neuroscience \cite{vicente2011transfer}, and biology \cite{tung2007inferring}. Transfer entropy has also been previously used with policy synthesis in MDPs in \cite{takashi17}. 

% Defining elaborate control objectives in MDPs have been considered in \cite{puterman2014}. However, this requires specifying complex reward functions in order to synthesize a control policy. To this end, temporal logic has been used as a formal way to allow the user to more intuitively specify high level specifications. Linear temporal logic (LTL) in particular has been popular for use in optimal control \cite{Svoreňová13,Fu15} as it is widely studied and several tools exist to synthesize provably correct policies in MDPs. It is known that maximizing the probability of satisfying LTL objectives in an MDP reduces to a reachability objective ~\cite{BaierKatoen08}. 

% The focus of this paper is to minimize the transfer entropy cost from state variables in an MDP that are deemed expensive to measure or transmit to the controller whilst maintaining a minimum probability of satisfying a mission objective specified in LTL.

% The focus of this paper is situations where the state is composed of multiple state variables measured by different sensors. This is often the case in applications such as planetary rovers where different sensors can have varying energy costs and is hence crucial to budget the use of certain sensors to conserve energy. This problem can be seen as analogous to the centralized multi-agent problem, as we only want to penalize knowledge of states from other agents but not our own. It can also be seen as simply penalizing the use of certain sensors over others.

% \paragraph*{\textbf{Related work}}
% This work builds on \cite{Takashi17} where transfer entropy is also used to quantify the rate of information flow from state to the control action. However, the authors do not consider situations where the state is composed of multiple state variables measured by different sensors, not all of which are remote or expensive to measure. This is often the case in applications such as planetary rovers where different sensors can have varying energy costs and is hence crucial to budget the use of certain sensors to conserve energy. This problem can be also seen as analogous to the centralized multi-agent problem where we only want to penalize knowledge of states from other agents but not our own to limit communication between agents. 

% Early work in \textit{minimal attention} controllers was done in \cite{Brockett97} and \cite{Brockett03} where the authors implement control actions that do not require much 'monitoring'. To do this an \textit{attention} cost is used that penalizes varying control away from a constant input. \cite{Chatterjee2013} provides algorithms in a 2-player game formulation with $\omega$-regular objectives to minimize communication bandwidth requirements between processors. This was %done by treating the game as an imperfect information game with additional \textit{attention} costs and was
% shown to be EXPTIME-complete making this solution computationally infeasible in many cases. Furthermore, the cost used is once again applied to changing control action and not directly to the underlying flow of information to the controller. In our case, we do not penalize changing control action, only the access to information and this does not penalize non-trivial control laws based on limited information from the state.

% There is also related work done in the fields of optimal active sensor placement and control. ~\cite{Hoffmann10,krause2006near} use mutual information to determine the optimal time or position to sense. This is the inverse problem studied to that posed in this paper as the authors are trying to \textit{maximize} an information-theoretic term (mutual information) so the most amount of information can be obtained with minimal sensor action. This is also case in ~\cite{Naiss13} where the authors maximize directed information. Other related work such as \cite{Tishby2011} uses a Kullback-Leibler cost (as opposed to transfer entropy) that quantifies the difference in the probability distributions of controlled trajectories compared to an uncontrolled.

% \paragraph*{\textbf{Contributions}}
% We provide a formal way to connect information theoretic techniques to compute a policy in a finite state MDP that minimizes the information flow (quantified by transfer entropy) to the controller with techniques from formal methods and probabilistic model checking by including linear temporal logic constraints. Hence, our contribution generalizes existing work and also allows for more complex objectives to be achieved. As far as we are aware there is no current work that minimizes the information-theoretic cost under temporal logic constraints. As a side note, we remark that the generalized formulation proposed in this paper is needed in order to make use of the formalism of linear temporal logic.

% In contrast to standard MDP policy computation under LTL constraints, information constrained MDPs often require randomized optimal strategies \cite{tanaka2017lqg,Todorov09} and in our case we will require a policy search under an infinite state space. To this end, we exploit the structure present in the problem to derive a sufficient condition to satisfy in the form of a coupled set of nonlinear equations. We then propose a numeric forward-backward algorithm similar to that proposed in ~\cite{Blahut72} to iteratively solve these nonlinear equations. We also present path planning experiments on grid worlds using moving and static obstacles and qualitatively analyze the impact of the information cost on relevant state variables.

% Additionally, in the application of networked control systems, we provide a physical interpretation of the transfer entropy cost problem formulation. We prove that it is in fact the lower bound on the data rate of the penalized state variables that must be transmitted in order to achieve a minimal threshold of performance.

% % The contributions of this work includes:
% % \begin{itemize}
% % \item Minimizing information flow (transfer entropy) to the controller from a user specified set of state variables under linear temporal logic constraints.
% % \item Using an Arimoto-Blahut algorithm to numerically solve the optimality equations to obtain the control policy.
% % \item In the application of networked control systems, we prove that the transfer entropy cost problem formulation is in fact the lower bound on the data rate from the part of the state space that is penalized under information cost that must be transmitted in order to achieve a minimal threshold of performance.
% % \item We provide path planning experiments on grid worlds using moving obstacles and qualitatively analyse the impact of the information cost on relevant state variables.
% % \end{itemize} 
% \paragraph*{\textbf{Structure}}
% The rest of this paper will be structured as follows. In section II, we present our definitions and other notation used in this paper. We then present the formal problem statement in section III. In section IV, we derive the optimality conditions and present the numeric algorithm to compute the optimal policy in section V. In Section V we study the practical meaning of the transfer entropy cost to networked control problems, We provide experiments to analyze the impact of the information cost in section VI and analyze its impact on path planning experiments on grid worlds. Finally, we conclude in Section VII and provide future direction.
% % We want to penalize the cost of information from a \emph{subset} of the state space. Second, we will extend this to synthesize joint controllers for multiple agents in a centralized fashion. Finally, we investigate the structure and will solve for controllers in a decentralized fashion by breaking up the centralized optimization problem.

% %The following notation will be used in this paper. The sequence $\left(x_1,x_{2}...x_t \right)$ is denoted $x^t$ and the subsequence $\left(x_k,x_{k+1}...x_l \)$ is denoted by $x_{l}^{k}$. 