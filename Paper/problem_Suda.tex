In this section, we present a method to solve an equivalent problem to \eqref{eqmainproblem} that explicitly takes into the account the mission specification.

Consider a finite labeled TEMDP $M=(\mathcal{\hat{X}},\mathcal{U},p,\AP,L)$ where, as before, the state space of $M$ is split into expensive and cheap to measure state variables $\mathcal{\hat{X}} = \mathcal{\bar{X}}_e \times \mathcal{\tilde{X}}_f$. We are additionally given a specification DFA $\mathcal{A}_{\varphi} = (\mathcal{S},s_I,2^{\AP}, \delta,\textrm{Acc})$, and finite time horizon $T$. The product MDP is $\mathcal{M}\defeq (\mathcal{V},\mathcal{U}, \Delta,v_0,L_{\varphi},\textrm{Acc}_{\mathcal{M}})$.  Hence, we will have the state space $\mathcal{V} = (\mathcal{\bar{X}}_e \times \mathcal{\tilde{X}}_f) \times S$. Now, for notational simplicity, we set $\mathcal{X} = \mathcal{V}$, the free to measure state $\mathcal{\tilde{X}} = (\mathcal{\tilde{X}}_f,\mathcal{S})$ (we assume without loss of generality that the state in the automaton is freely known), and the expensive to measure state $\mathcal{\bar{X}} = \mathcal{\bar{X}}_e$. Let $X = (\bar{X}_e,\tilde{X}_f,S)$ and $x = (\bar{x}_f,\tilde{x}_s,s)$ be defined similarly. Thus, our state space is now $\mathcal{X}= \mathcal{\bar{X}} \times \mathcal{\tilde{X}}$ with random variable $X = (\bar{X},\tilde{X})$. 

We define a state-action cost in the product MDP in the following way. We define a function $c_t(x_t,u_t,x_{t+1})$, such that for every transition from $x_t$ to $x_{t+1}$, the cost is $0$ if neither $x_t$ or $x_{t+1}$ are in $\textrm{Acc}_{\mathcal{M}}$. The cost is $-1$ if $x_t \notin \textrm{Acc}_{\mathcal{M}}$ and $x_{t+1} \in \textrm{Acc}_{\mathcal{M}}$ and no state in $\textrm{Acc}_{\mathcal{M}}$ has been visited prior to reaching $x_t$.  Intuitively, minimizing this quantity will result in a policy $q$ that maximizes the probability of reaching $\textrm{Acc}_{\mathcal{M}}$ and hence, equivalently will maximize the probability of satisfying the temporal logic specification in $M$. The expected accumulated reward from state $x_0$ given by $\sum_{t=0}^{T-1}\mathbb{E}\{c_t(x_t,u_t,x_{t+1})\}$ will equal the \emph{negative} of the reachability probability to the target set $C$ in $T-$steps \ie we have
\vspace{-0.2cm}
\begin{align}\label{eqn:cost}
\sum_{t=0}^{T-1}\mathbb{E}\{c_t(x_t,u_t,x_{t+1})\} = -h^{\leq T}(x,\textrm{Acc}_{\mathcal{M}})
\end{align}.
\vspace{-0.3cm}
% First we define a reward function $R(v_t,u_t,v_{t+1})$, such that for every transition from $v_t$ to $v_{t+1}$, the reward is $0$ if neither $v_t$ or $v_{t+1}$ are in $C$. The reward is $1$ if $v_t \notin C$ and $v_{t+1} \in C$ and no state in $C$ have been visited prior to reaching $v_t$. Put simply, maximizing this quantity will result in a policy $q$ that maximizes the probability reaching $C$ and hence, equivalently will maximize the probability of satisfying the LTL specification $\varphi$ in $M$. The expected accumulated reward from state $v_0$ given by $\sum_{t=0}^{T-1}\mathbb{E}\{R(v_t,u_t,v_{t+1})\}$ will equal the T-step state value defined earlier. However, in this paper we are interested in solving a cost minimization problem. To do this, we define a cost function $c(v_t,u_t,v_{t+1}) = -R(v_t,u_t,v_{t+1})$. Hence, our \emph{T-step state value} under a policy $q$ will be

% \begin{align*}
% W^{q}_{\mathcal{M}} \defeq  \sum_{t=0}^{T-1}\mathbb{E}\{c(v_t,u_t,v_{t+1})\}
% \end{align*}

% which is actually the \emph{negative} of the reachability probability to the target set $C$. 

% \paragraph*{Optimal T-step policy} The optimal T-step value of the product MDP defined previously is given by $W_{\mathcal{M}}^{*}(v,T) =  \min_{q}W^q_{\mathcal{M}}(v,T)$ and the optimal T-step policy is $q^* = \argmin_{q}W^q_{\mathcal{M}}(v,T)$

% Assume now we have divided our MDP state space into an expensive-to-measure and free-to-measure state variables $\mathcal{X} = \mathcal{X}_e \times \mathcal{X}_f$. We want to find a policy $q$ in the product MDP $\mathcal{M}$ that uses minimizes the directed information from $\mathcal{X}_e$ to the policy $q$ whilst ensuring the LTL specification will be satisfied to a minimal probability threshold.

% The standard problem is to compute an optimal policy $q_t(u_t|x^t,u^{t-1})$ that minimizes the cost function as shown in equation (\ref{eqn:optpol}). We also want to minimize the rate of communication which is shown in equation (6). 

Setting $J(X^T,U^{T-1}) = \sum_{t=0}^{T-1}\mathbb{E}\{c_t(x_t,u_t,x_{t+1})\}$, we recover the formulation of \eqref{eqmainproblem}.
%We will have cost functional $J(X^T,U^{T-1}) \defeq - \sum_{t=0}^{T-1}{\mathbb{E}\{c(x_t,u_t)\}}$. Note the negative sign means that $J(X^T,U^{T-1})$ is the expected reachability probability to the target set from the initial state $x_0$. Hence we solve the optimization problem in \eqref{eqmainproblem}, on the state space of the product MDP with the cost being the negative reachability to the accepting end component. 

\paragraph*{Remark} The constrained optimization problem in equation (\ref{eqmainproblem}) can be written as a \emph{Lagrangian relaxation} in the following way

\begin{align}\label{eqn:constopt}
T_{m,n}(D) & \defeq \min_{\{q_t\}_{t=1}^T}  J(X^{T},U^{T-1}) + \beta I(\bar{X}^T \rightarrow U^{T-1}||\bar{X}^T)
\end{align}
where $\beta$ is a positive constant

Intuitively, this means that we want to minimize the information flow from the state variables in $\mathcal{\bar{X}}$ subject to the constraint on the accumulated cost $J$. Using the cost function defined in \eqref{eqn:cost}, this constrains the probability of not satisfying the specification.

% We will prove in the following that the transfer entropy cost used is a fundamental lower bound for the minimal communication rate $R(D)$, in other words, we prove that $R(D) \geq T_{m,n}(D)$. The consequence of this is that the obtained optimal control policy $\{q_t\}_{t=1}^T$ obtained as the solution to equation ($\ref{eqn:constopt}$) is one that requires a minimal data rate to implement by the controller while still achieving a control performance of at least $D$. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%