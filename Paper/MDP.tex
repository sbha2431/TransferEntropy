% \paragraph{Markov decision processes}
% 	An \emph{MDP} is a tuple $M=(\mathcal{X},\mathcal{U},p)$ where $\mathcal{X}$ is a
% 	finite set of \emph{states},
% 	$\mathcal{U}$ is a finite alphabet of \emph{actions},
% 	$p: S\times\mathcal{U} \to \mathcal{D}({\mathcal{X}})$ is a (partial) \emph{probabilistic
% 	transition function} that assigns to a state $x\in \mathcal{X}$ and an action $u \in
% 	\mathcal{U}$ a probability distribution over the successor states. We
% 	abbreviate $p(x_{t},u)(x_{t+1})$ by $p(x_{t+1}|x_t,u_t)$.
\paragraph*{Labeled Markov decision process (MDP)} Consider a set $\AP$ of \emph{atomic propositions} which can be used, for example, to mark a state as being a ``faulty configuration'' (reaching it is, thus, undesirable). A \emph{labeled MDP} is an MDP whose states are labeled with atomic propositions. More formally, it is a tuple $M=(\mathcal{X},\mathcal{U},p,\AP,L)$ where $\mathcal{X}$ is a
	finite set of \emph{states},
	$\mathcal{U}$ is a finite alphabet of \emph{actions},
	$p: \mathcal{X}\times\mathcal{U} \to \mathcal{D}({\mathcal{X}})$ is a \emph{probabilistic
	transition function} that assigns, to a state $x\in \mathcal{X}$ and an action $u \in
	\mathcal{U}$, a probability distribution over the successor states. We
	abbreviate $p(x_{t},u)(x_{t+1})$ by $p(x_{t+1}|x_t,u_t)$.
$L : X \rightarrow 2^{\AP}$ is the \emph{labeling function} which indicates the set of atomic propositions which are true in each state of the MDP.
\paragraph*{Runs and policies}
A \emph{run} from state $x_0$ with time horizon $T$ is a sequence $\rho = x_0 u_0 x_1 u_1 \dots ,x_{T-1},u_{T-1},x_{T}$ of states and actions such that for all $0 \leq t\leq T$ we have $p(x_{t+1}|x_t,u_t)>0$. 
%
A \emph{policy} corresponds to a way of selecting actions based on the history
of states and actions. While \emph{deterministic stationary} policies
are known to be sufficient for certain classes of problems, such as pure reachability ~\cite{puterman2014}, policies in general can be non-deterministic and history dependent. In this paper, we consider the general form and formally represent a policy as a conditional probability distribution $q_t(u_t|x^t,u^{t-1})$. 

\paragraph*{Markov chain}
A Markov chain is a tuple $(\mathcal{X},x_I,p)$ where $\mathcal{X}$ is (in our case) a finite set of states, $x_I \in \mathcal{X}$ is the initial state, and $p: \mathcal{X} \to \dist{Q}$ is a probabilistic transition function. An MDP $M$ together with a policy $q$ induces a \emph{Markov chain} $M^q$.  Notions of runs in a Markov chain are the same as those defined earlier. 

Given a Markov chain $M^q = (\mathcal{X},x_I,p)$, the state visited at the step $t$ is
a random variable. We denote by $h^{k}(x,\mathcal{B})$ the probability that a
run starting from state $x$ visits the set $\mathcal{B}$ in exactly $k$ steps. By definition
$h^{\leq i}(x,\mathcal{B}) = \sum_{k=0}^{i} h^{k}(x,\mathcal{B})$ where $h^0(x,\mathcal{B})$ is $0$ if $x
\not\in \mathcal{B}$ and $1$ otherwise. %Furthermore, in the infinite horizon setting,
%$h(x,\mathcal{B}) = \sum_{k=0}^{\infty}h^{k}(x,\mathcal{B})$.

%A run $\rho$ is \emph{consistent} with a policy $q$ if it can be
%obtained by extending its prefixes using $q$. Formally, $\rho=x_0
%u_0 x_1 u_1 \dots$ is consistent with $x$ if for all $t \ge 0$ we have that
%$u_t \in \{u| q_t(u|x^t,u^{t-1} > 0)\}$ and $p(x_{t+1}|x_t,u_t)>0$

%\paragraph*{End components}
%An \textit{end component} of an MDP $M=(\mathcal{X},\mathcal{U},p)$
%is a pair $(\mathcal{B},\alpha)$ where $\mathcal{B} \subseteq \mathcal{X}$ and 
%$\alpha : \mathcal{B} \to 2^{\mathcal{U}}$ is a mapping from states to actions such that, by
%playing an action $\alpha(x)$ from state $x \in \mathcal{B}$, with probability $1$ the
%next state reached will also be in $T$. More formally, we require that
%for all $x \in \mathcal{B}$ it holds that
%\begin{itemize}
%	\item $\alpha(x) \in \mathcal{U}$ is non-empty;
%	\item if there are $x_t \in \mathcal{X}$ and $u \in \alpha(x)$ such that
%		$p(x_{t+1} \vert x_t,u_t ) >0$ then $x_{t+1} \in \mathcal{B}$;
%	\item for all $x,x' \in \mathcal{B}$ there is a run from $x$ going to $x'$ and a run going from $x'$ to $x$.
%\end{itemize}
%End components in an MDP can be found using graph analysis techniques ~\cite{BaierKatoen08}.